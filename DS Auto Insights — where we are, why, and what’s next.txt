DS Auto Insights — where we are, why, and what’s next
The problem we’re solving
Non-technical PMs and lean teams struggle to turn raw CSVs/tables into decisions quickly. Existing BI tools (Power BI/Tableau) are great at dashboards but require modeling and setup; chat-first “AI BI” tools often hallucinate or can’t actually operate on the data. Our goal is a PM-first, chat-native analyst that:
* ingests ad-hoc datasets (CSV/XLSX now; SQL later),

* understands lightweight business context,

* plans safe analysis steps,

* executes real computations on the dataframe,

* explains results and exports them.

Guiding principles
   * Safety over magic: the model doesn’t “read the whole dataset”; it calls whitelisted tools that operate on the dataframe.

   * Transparency: show the plan/steps and intermediate results where helpful.

   * MVP focus: minimize UI, maximize usefulness (summary, trends, simple viz, Q&A).

   * Modular: Streamlit frontend, small Python utilities, pluggable model/tool layer.

________________


What we’ve built (and why)
1) Project + environment
      * Poetry-managed repo, Python 3.11 (to keep Streamlit + LangChain happy).

      * .env loading + sidebar diagnostics (quick visibility on API keys).

Why: predictable env, fast iteration, fewer “it works on my machine” issues.
2) Streamlit app (fast full-stack MVP)
         * File uploader for CSV/XLSX with robust parsing (load_table()), memory usage, and a Columns & dtypes preview.

         * Tabs:

            * Chat (LLM): multi-turn chat UI, chat history, a “thinking…” placeholder, and agent execution.

            * Summary: dataset summary (rows/cols, missing totals), numeric describe, per-column missing, top categories.

            * Explore: simple numeric correlation matrix + suggested visualizations (histograms/top-cats).

Why: give PMs instant value without setup (upload → insight), and a familiar chat mental model.
3) Analysis utilities (util.py)
               * load_table() handles CSV/XLSX variants, encodings, bad lines, etc.

               * summarise_dataset(), key_trends_numeric_only() (corr table), suggest_visualisations().

Why: repeatable, debuggable primitives the UI and agents can call.
4) Tool-calling agent (LangChain + OpenAI)
                  * RunPandasQueryTool: a safe read-only tool that lets the LLM run simple, pure pandas expressions on df (e.g., df["xG"].mean(), df[["goals","xG"]].corr()), with guardrails (block import/exec/eval/os/subprocess, etc.).

                  * Planner/agent wrapper (planner_mcp.py): uses ChatOpenAI (e.g., gpt-4o) with a tool-calling agent that decides when to call the pandas tool vs. answer directly.

                  * Chat flow: when you ask a question, we queue → rerun → show “thinking” → run the agent → display answer, so the input stays at the bottom and UX feels real-time.

Why: this keeps the model grounded in real computations and lets us scale toolset safely (more tools later).
5) Narrative & export (earlier work)
                     * Markdown export (exporter.py) and a narrative function were prototyped; tabulate added for pretty tables.

Why: PMs need shareable, light-weight outputs (“drop into a doc/slack”).
(We can re-wire export/narrative to the new chat flow when you want.)
________________


What changed along the way (learnings)
                        * Dropped brittle time-series heuristics (e.g., guessing a “time” column). It’s dataset-specific and annoyed users. We’ll bring time handling back as an explicit config/tool, not guesswork.

                        * Planner V1 → Tool-calling: initial “plan JSON + executor” worked but was rigid and prone to quota/parse hiccups. Tool-calling via LangChain is simpler for the MVP and more extensible.

                        * Safety over flexibility: we block arbitrary Python; only allow simple pandas reads/aggregations. This reduces risk and keeps results deterministic.

________________


What’s working end-to-end right now
                           * Upload CSV/XLSX → preview schema → get Summary/Explore insights.

                           * Chat with your data: ask questions; the agent can compute correlations, aggregates, simple filters, etc., and reply in natural language.

                           * Good UX: chat history, “thinking…” widget, input pinned at bottom, diagnostics in sidebar.

________________


What’s next (near-term roadmap)
Week 1–2: Polish & reliability
                              * Harden the pandas tool: expand the allow-list (groupby, describe, corr, value_counts, quantiles), better error messaging.

                              * Add a small library of higher-level tools (whitelisted ops):

                                 * groupby_aggregate(group, metric, agg)

                                 * histogram(column)

                                 * top_categories(column, n)

                                 * describe_numeric() / correlation_matrix()

                                    * Chart rendering tools: return data + chart hints so the UI can render Streamlit charts consistently.

                                    * Re-enable Export/Narrative from chat results (click → Markdown file).

Week 3–4: Data connectivity & context
                                       * Context slotting: an optional spec/notes box that the agent sees every turn (e.g., “single player”, “rows=events”, “date tz=UTC”).

                                       * SQL connectors (behind a flag): read-only queries to Postgres/BigQuery/Snowflake (start with SQLite demo). Keep the same tool-calling paradigm.

                                       * Cached profiling: faster repeat questions; surface “interesting columns” automatically (high variance, high correlation, skew, cardinality).

Month 2: Agent robustness & UX
                                          * LangGraph state machine: fewer parse loops, better retries, and deterministic flows.

                                          * Model routing: abstract provider so you can switch gpt-4o ↔ Claude Sonnet ↔ Gemini safely.

                                          * Cost controls: token budgets, sampling temperature caps, and simple per-session spend estimates.

                                          * Saved “Projects”: persist uploaded data, context, and chat transcripts; quick re-runs; share links.

Later: MCP “for real”, governance, & teams
                                             * True MCP tool servers for data sources (GSheets, S3, SQL) and artifacts (files/plots).

                                             * Policy & governance: PII redaction, query audit logs, role-based access.

                                             * Templates: cohort/retention, funnel, feature-usage, churn/activation, LTV/CAC—one-click analyses with explainers.

________________


How this product is different
                                                * PM-first: starts from plain files + questions; doesn’t require a semantic layer upfront.

                                                * Agent that actually computes: no “hand-wavy” answers; all insights come from operations on the uploaded dataframe.

                                                * Transparent & extensible: see steps; add safe tools incrementally; export outcomes.

                                                * Model-agnostic future: we can swap LLMs and keep the tool layer stable.

________________


Immediate next 5 tasks (practical)
                                                   1. Add 3–4 first-class tools (groupby, histogram, top-cats, corr) and have the agent prefer them over free-form pandas.

                                                   2. Wire Export/Narrative to the chat’s last result.

                                                   3. Add chart hints to tool results so the UI renders consistent visuals.

                                                   4. Build a tiny eval set (10 questions over 2–3 sample datasets) to catch regressions.

                                                   5. Add a config panel: model choice, max rows sent to the model, and a “safe mode” toggle.

If you want, I can generate those tool stubs next and slot them into the agent so your chat can do richer, safer analyses out of the box.
                                                      *